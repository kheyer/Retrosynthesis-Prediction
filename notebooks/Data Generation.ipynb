{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMILES Augmented Data Generation\n",
    "\n",
    "The core dataset used for these tests was generated by [Liu et al](https://pubs.acs.org/doi/10.1021/acscentsci.7b00303) and is available at [here](https://github.com/pandegroup/reaction_prediction_seq2seq/tree/master/processed_data).\n",
    "\n",
    "The function used for SMILES enumeration was taken from [this repo](https://github.com/EBjerrum/SMILES-enumeration) with minor adaptations for Python3 compatibility.\n",
    "\n",
    "The code below generates three augmented datasets with 4x, 16x and 40x augmentation over the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SmilesEnumerator import SmilesEnumerator\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Augmentation\n",
    "\n",
    "This script generated four versions of each input datapoint. Specifically, one augmented source sequence and one augmented target sequence will be generated and combined with the un-augmented source and target sequences:\n",
    "\n",
    "`source + target\n",
    "source_augmented + target\n",
    "source + target_augmented\n",
    "source_augmented + target_augmented`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('G:/reaction_prediction_seq2seq/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallAug():\n",
    "    def __init__(self, path):\n",
    "        train_source = list(open(path/'processed_data/train_sources'))\n",
    "        train_targs = list(open(path/'processed_data/train_targets'))\n",
    "        self.data = list(zip(train_source, train_targs))\n",
    "        \n",
    "    def augment(self, save_folder_path):\n",
    "        self.generate_augs(self.data)\n",
    "        self.save_df(save_folder_path)\n",
    "        \n",
    "    def generate_augs(self, data):\n",
    "        with ThreadPoolExecutor(8) as ex:\n",
    "            new_data = ex.map(lambda x: self.augment_rxn(x), data)\n",
    "            \n",
    "        aug_data = list(new_data)\n",
    "        self.df = pd.DataFrame(columns=['Source', 'Target', 'rxn_number'])\n",
    "        \n",
    "        for i in range(len(aug_data)):\n",
    "            df_i = pd.DataFrame(aug_data[i], columns=['Source', 'Target'])\n",
    "            df_i['rxn_number'] = i\n",
    "            self.df = self.df.append(df_i)\n",
    "            \n",
    "        self.df.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "    def save_df(self, path):\n",
    "        self.df.to_csv(path/'augmented_df.csv', index=False)\n",
    "        \n",
    "        sources_aug = list(self.df.Source.values)\n",
    "        with open(path/'train_sources_augmented.txt', 'w') as f:\n",
    "            for sa in sources_aug:\n",
    "                rxn, smile = sa.split(' ')\n",
    "                smile_tok = ' '.join([i for i in smile])\n",
    "                f.write(rxn + ' ' + smile_tok + '\\n')\n",
    "                \n",
    "        targets_aug = list(self.df.Target.values)\n",
    "        with open(path/'train_targets_augmented.txt', 'w') as f:\n",
    "            for ta in targets_aug:\n",
    "                smile_tok = ' '.join([i for i in ta])\n",
    "                f.write(smile_tok + '\\n')\n",
    "                \n",
    "        \n",
    "    def augment_rxn(self, data):\n",
    "        source = data[0].strip('\\n')\n",
    "        targ = data[1].strip('\\n')\n",
    "\n",
    "        sme = SmilesEnumerator()\n",
    "        new_data = []\n",
    "\n",
    "        rxn_class = source.split(' ')[0]\n",
    "\n",
    "        source_smile = ''.join(source.split('> ')[1].split(' '))\n",
    "        targ_smile = ''.join(targ.split(' '))\n",
    "\n",
    "        source_aug = rxn_class + ' ' + sme.randomize_smiles(source_smile)\n",
    "        source_smile = rxn_class + ' ' + source_smile\n",
    "        targ_aug = sme.randomize_smiles(targ_smile)\n",
    "\n",
    "        new_data = [[source_smile, targ_smile],\n",
    "                    [source_aug, targ_smile],\n",
    "                    [source_aug, targ_aug],\n",
    "                    [source_smile, targ_aug]]\n",
    "\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SmallAug(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.augment(path/'augmenteed_data_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium Augmentation\n",
    "\n",
    "The logic here is similar to above. This time we pass an `n_augs` parameter that controls the number of augmented datapoints generated. The class generates `n_augs` augmented datapoints plus the original datapoint for a total of `n_augs + 1` datapoints.\n",
    "\n",
    "For medium augmentation we use `n_augs = 15`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MediumAug():\n",
    "    def __init__(self, path, n_augs):\n",
    "        train_source = list(open(path/'processed_data/train_sources'))\n",
    "        train_targs = list(open(path/'processed_data/train_targets'))\n",
    "        self.data = list(zip(train_source, train_targs))\n",
    "        self.n_augs = n_augs\n",
    "        \n",
    "    def augment(self, save_folder_path):\n",
    "        self.generate_augs(self.data)\n",
    "        self.save_df(save_folder_path)\n",
    "        \n",
    "    def generate_augs(self, data):\n",
    "        with ThreadPoolExecutor(8) as ex:\n",
    "            new_data = ex.map(lambda x: self.augment_rxn(x), data)\n",
    "            \n",
    "        aug_data = list(new_data)\n",
    "        self.df = pd.DataFrame(columns=['Source', 'Target', 'rxn_number'])\n",
    "        \n",
    "        for i in range(len(aug_data)):\n",
    "            df_i = pd.DataFrame(aug_data[i], columns=['Source', 'Target'])\n",
    "            df_i['rxn_number'] = i\n",
    "            self.df = self.df.append(df_i)\n",
    "            \n",
    "        self.df.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "    def save_df(self, path):\n",
    "        self.df.to_csv(path/'augmented_df.csv', index=False)\n",
    "        \n",
    "        sources_aug = list(self.df.Source.values)\n",
    "        with open(path/'train_sources_augmented.txt', 'w') as f:\n",
    "            for sa in sources_aug:\n",
    "                rxn, smile = sa.split(' ')\n",
    "                smile_tok = ' '.join([i for i in smile])\n",
    "                f.write(rxn + ' ' + smile_tok + '\\n')\n",
    "                \n",
    "        targets_aug = list(self.df.Target.values)\n",
    "        with open(path/'train_targets_augmented.txt', 'w') as f:\n",
    "            for ta in targets_aug:\n",
    "                smile_tok = ' '.join([i for i in ta])\n",
    "                f.write(smile_tok + '\\n')\n",
    "\n",
    "    \n",
    "    def augment_rxn(self, data):\n",
    "        source = data[0].strip('\\n')\n",
    "        targ = data[1].strip('\\n')\n",
    "\n",
    "        sme = SmilesEnumerator()\n",
    "        new_data = []\n",
    "\n",
    "        rxn_class = source.split(' ')[0]\n",
    "\n",
    "        source_smile = ''.join(source.split('> ')[1].split(' '))\n",
    "        targ_smile = ''.join(targ.split(' '))\n",
    "\n",
    "        source_aug = [rxn_class + ' ' + sme.randomize_smiles(source_smile) for i in range(self.n_augs)]\n",
    "        source_aug += [rxn_class + ' ' + source_smile]\n",
    "\n",
    "        targ_aug = [sme.randomize_smiles(targ_smile) for i in range(15)]\n",
    "        targ_aug += [targ_smile]\n",
    "\n",
    "        new_data = [[s,t] for s,t in zip(source_aug, targ_aug)]\n",
    "\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = MediumAug(path, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.augment(path/'augmenteed_data_medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Augmentation\n",
    "\n",
    "Larger scale augmentation requires a little extra logic. When generating a large number of augmented sequences randomly, there is a good chance the same augmented variant will appear more than once. This class generates `4 * n_aug` augmented variants, reduces the set of augmented SMILES to a unique set, then pulls `n_aug` samples from the unique set.\n",
    "\n",
    "For large augmentation, we use `n_augs = 40`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeAug():\n",
    "    def __init__(self, path, n_augs):\n",
    "        train_source = list(open(path/'processed_data/train_sources'))\n",
    "        train_targs = list(open(path/'processed_data/train_targets'))\n",
    "        self.data = list(zip(train_source, train_targs))\n",
    "        self.n_augs = n_augs\n",
    "        \n",
    "    def augment(self, save_folder_path):\n",
    "        self.generate_augs(self.data)\n",
    "        self.save_df(save_folder_path)\n",
    "        \n",
    "    def generate_augs(self, data):\n",
    "        with ThreadPoolExecutor(8) as ex:\n",
    "            new_data = ex.map(lambda x: self.augment_rxn(x), data)\n",
    "            \n",
    "        aug_data = list(new_data)\n",
    "        self.df = pd.DataFrame(columns=['Source', 'Target', 'rxn_number'])\n",
    "        \n",
    "        for i in range(len(aug_data)):\n",
    "            df_i = pd.DataFrame(aug_data[i], columns=['Source', 'Target'])\n",
    "            df_i['rxn_number'] = i\n",
    "            self.df = self.df.append(df_i)\n",
    "            \n",
    "        self.df.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "    def save_df(self, path):\n",
    "        self.df.to_csv(path/'augmented_df.csv', index=False)\n",
    "        \n",
    "        sources_aug = list(self.df.Source.values)\n",
    "        with open(path/'train_sources_augmented.txt', 'w') as f:\n",
    "            for sa in sources_aug:\n",
    "                rxn, smile = sa.split(' ')\n",
    "                smile_tok = ' '.join([i for i in smile])\n",
    "                f.write(rxn + ' ' + smile_tok + '\\n')\n",
    "                \n",
    "        targets_aug = list(self.df.Target.values)\n",
    "        with open(path/'train_targets_augmented.txt', 'w') as f:\n",
    "            for ta in targets_aug:\n",
    "                smile_tok = ' '.join([i for i in ta])\n",
    "                f.write(smile_tok + '\\n')\n",
    "\n",
    "    \n",
    "    def augment_rxn(self, data):\n",
    "        source = data[0].strip('\\n')\n",
    "        targ = data[1].strip('\\n')\n",
    "        augs = self.n_augs * 4\n",
    "\n",
    "        sme = SmilesEnumerator()\n",
    "        new_data = []\n",
    "\n",
    "        rxn_class = source.split(' ')[0]\n",
    "\n",
    "        source_smile = ''.join(source.split('> ')[1].split(' '))\n",
    "        targ_smile = ''.join(targ.split(' '))\n",
    "\n",
    "        source_aug = list(set([sme.randomize_smiles(source_smile) for i in range(augs)]))\n",
    "\n",
    "        if len(source_aug) > self.n_augs:\n",
    "            source_aug = random.sample(source_aug, self.n_augs)\n",
    "\n",
    "        targ_aug = list(set([sme.randomize_smiles(targ_smile) for i in range(augs)]))\n",
    "\n",
    "        if len(targ_aug) > self.n_augs:\n",
    "            targ_aug = random.sample(targ_aug, self.n_augs)\n",
    "\n",
    "        source_aug = [rxn_class + ' ' + i for i in source_aug]\n",
    "\n",
    "        for s, t in zip(source_aug, targ_aug):\n",
    "            new_data.append([s,t])\n",
    "\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = LargeAug(path, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.augment(path/'augmenteed_data_large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
